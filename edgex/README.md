# EdgeX Configuration

This folder contains configuration for EdgeX components.

## Regenerating Configuration

The subdirectories `claims`, `deployments`, `policies`, `services`, and
`volumes` contain Kubernetes resource configuration files. Most of these
were generated using [kompose](https://kompose.io/) from a docker-compose
file which was, in turn, generated using the
[edgex-compose](https://github.com/edgexfoundry/edgex-compose) tools.

The process can be redone, e.g. to make changes to the EdgeX services in
use, with the following steps.

NOTE: The first part of this process creates a Kubernetes "pod" for each
EdgeX microservice, and one or more "service" definitions for each. These
service and pod definitions are not defined uniquely per-node, so this
configuration will only support one edge node. The second part of the
process, starting with the "Consolidate Microservices" section, puts all the
microservices together in a single pod with one service definition providing
access, and also makes each worker node have a unique pod and service
definition, suitable for a cluster with multiple edge nodes.

### Check Out EdgeX-Compose

The following command checks out the edgex-compose repository into a folder
named `edgex-compose` and switches to the Jakarta (2.1.0) release version.

NOTE: Perform this in a separate directory, outside of this git workspace,
e.g. in your home directory.

```
git clone https://github.com/edgexfoundry/edgex-compose.git
cd edgex-compose
git checkout v2.1.0
```

### Build The Docker-Compose File

In the edgex-compose workspace, change to the `compose-builder` folder and
build a new docker-compose file.

```
cd compose-builder
make gen no-secty ds-virtual asc-mqtt
```

This will replace the `docker-compose.yml` file in the `compose-builder`
directory with the customized version.

### Install Kompose

If it is not already installed, install Kompose on your system by
downloading it with the commands below.

```
curl -L https://github.com/kubernetes/kompose/releases/download/v1.25.0/kompose-linux-amd64 -o kompose
chmod +x kompose
sudo mv ./kompose /usr/local/bin/kompose
```

### Generate Kubernetes Resources

Copy the `docker-compose.yml` file generated above to a new directory and
run the `kompose convert` command.

```
kompose convert
```

This will create a large number of `.yaml` files for resources required by
EdgeX.

### Correct Resource Files

The files generated by Kompose will not work as-is. Certain manual fixes are
required.

* Delete the `system-claim0-persistentvolumeclaim.yaml` file.
* In `system-deployment.yaml` replace the `persistentVolumeClaim` with
  a `hostPath`:

```
<           persistentVolumeClaim:
<             claimName: system-claim0
---
>           hostPath:
>             path: /var/run/docker.sock
>             type: Socket
```

* For each `*-service.yaml` and `*-deployment.yaml` pair, change the 
  `metadata:` `name:` in the service file to match the hostname of the
  container in the deployment file. For example, if
  `app-service-mqtt-export-deployment.yaml` contains
  `hostname: edgex-app-mqtt-export`, change
  `name: app-service-mqtt-export` in `app-service-mqtt-export-service.yaml` to
  `name: edgex-app-mqtt-export`

### Copy The Files

The resource files need to be moved into subdirectories of the `edgex`
directory before they can be used.

* The files ending with `persistentvolumeclaim.yaml` go in the `claims` subdirectory.
* The files ending with `deployment.yaml` go in the `deployments` subdirectory.
* The files ending with `service.yaml` go in the `services` subdirectory.
* `edgex-network-networkpolicy.yaml` goes in the `policies` subdirectory.
* Note that the `volumes` directory contains definitions of local storage
  volumes for use by the persistent volume claims. (If the set of
  persistent volume claims have been changed, these volumes must be updated
  to match.)

### Consolidate Microservices

Once the steps up to this point have been completed it should be possible to
start a cluster with a single edge node. The steps below make it possible to
support multiple edge nodes in a cluster.

Move the `services` and `deployments` files produced above to a backup, e.g.
by renaming the directories. In the `services` directory create a single
`edgex-service.yaml` file, and copy the `ports` from `spec` in each of the
old service files to a single `ports` list in the new service. Name each
port appropriately, e.g. using the `name` from the `metadata` of the old
service. For services that had more than one port, e.g. `data-service.yaml`,
you can add the port number to the end of the name, like `core-data-59880`.

```
apiVersion: v1
kind: Service
metadata:
  name: edgex-NODENAME
spec:
  ports:
  - name: consul
    port: 8500
    targetPort: 8500
    protocol: TCP
  - name: redis
    port: 6379
    targetPort: 6379
    protocol: TCP
...
```

The name of the service and the selector should be `edgex-NODENAME`. The
Ansible playbooks for starting EdgeX will replace `NODENAME` with the name
of the edge node.

In a similar way, create a single `edgex.yaml` file in the `deployments`
directory. The `spec: template: spec: containers:` list should contain all
the containers from each of the old deployments files. The list of `volumes:`
in the `spec: template: spec:` is also made up of all the volumes in each of
the old deployments, with the `claimName` field of any `persistentVolumeClaim`
with `-NODENAME` added to the end.

In the `env:` list of each container's definition, replace the `value` of
all the environment variables naming a hostname from another microservice
with `localhost`. For containers from `edgexfoundry`, also add the following
environment variable to the `env:` list.

```
            - name: EDGEX_CONFIGURATION_PROVIDER
              value: "consul.http://localhost:8500"
```
Add the `ServerBindAddr` of the interface that the service's REST server 
should listen in key-value format. A value of `0.0.0.0` means listen on 
all available interfaces.

```
            - name: SERVICE_SERVERBINDADDR
              value: 0.0.0.0
```

The final deployment file should resemble this:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edgex-NODENAME
  labels:
    name: edgex-NODENAME
spec:
  replicas: 1
  selector:
    matchLabels:
      app: edgex-NODENAME
      name: edgex-NODENAME
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: edgex-NODENAME
        name: edgex-NODENAME
    spec:
      containers:
        - name: edgex-core-consul
          args:
            - agent
            - -ui
            - -bootstrap
            - -server
            - -client
            - 0.0.0.0
          image: consul:1.10.3
...
        - name: edgex-redis
...
        - name: edgex-support-scheduler
          env:
            - name: CLIENTS_CORE_COMMAND_HOST
              value: localhost
            - name: CLIENTS_CORE_DATA_HOST
              value: localhost
...
            - name: EDGEX_CONFIGURATION_PROVIDER
              value: "consul.http://localhost:8500"
            - name: SERVICE_SERVERBINDADDR
              value: 0.0.0.0
...

      volumes:
        - name: db-data
          persistentVolumeClaim:
            claimName: db-data-NODENAME
...
      nodeSelector:
        kubernetes.io/hostname: NODENAME
```

Finally, modify the persistent volumes and claims to be assigned to each node.
Change the `name:` in each of the files in `claims` to add `-NODENAME`. 
Change the `name:` in each file in `volumes` to match, and change the
`values:` in the `matchExpressions` to `NODENAME`.

With these changes made, it should be possible to run a cluster with multiple
edge nodes.

### Update The Local Docker Registry

If any images have changed (or been added) the local Docker registry will
need to be updated. Run the `pull_upstream_images.yml` playbook in
`deploy/playbook` to do this.

```
ansible-playbook -i ./hosts pull_upstream_images.yml
```

To save disk space on the master node you may want to shut down and delete
the local Docker registry to remove old images once before doing this.

# Enabling EdgeX Security Features

The procedure outlined above is for the case of recreating the Kubernetes
deployment with EdgeX's security features disabled (the `no-secty` option to
`make gen` in edgex-compose). If the security features are turned on, the
process is basically the same, but there are some workarounds required for
issues in the additional micro-services used for security. These workarounds
are outlined below.

## Workarounds

This folder also contains workarounds for the following issues encountered
when building configuration with security enabled:

* Github edgex-go issue [#3851](https://github.com/edgexfoundry/edgex-go/issues/3851)
* Github edgex-go issue [#3852](https://github.com/edgexfoundry/edgex-go/issues/3852)
* A similar issue with the "kong" volume in the "kong" container hiding the `/usr/local/kong` directory, making Kong unable to start
* An issue with Kong iteself, where the host name "localhost" could not be resolved to an IP address in spite of `/etc/hosts` containing the appropriate `127.0.0.1` value

There is also a workaround for the following issue which does not prevent the
pod from initializing, and is a result of our decision to combine all EdgeX
containers into a single pod:

* The `security-proxy-setup` container exits on succesful completion, but all other containers are expected to run indefinitely and Kubernetes does not support different restart policies for containers in the same pod, resulting in the `security-proxy-setup` container being restarted constantly

Note: This issue appears to be addressed by Github edgex-go issue
[#3917](https://github.com/edgexfoundry/edgex-go/pull/3917), and fixed in the
v2.2.0-dev.43 version of edgex-go.

The workarounds are implemented by building three new images based on images
used in the original EdgeX Foundry docker-compose file, and using the
replacements in our Kubernetes deployment definition. The three containers
are built using `Makefile` and `Dockerfile` in the directories below:

* `edgex/security-bootstrapper`
* `edgex/security-secretstore-setup`
* `edgex/kong`

### Almost No Containers Start With Security Enabled (Issue 3851)

When security is enabled the EdgeX Foundry docker-compose file translated
into a Kubernetes deployment supplies a `command:` overriding the entrypoint
of the container and running a script in `/edgex-init`. The `/edgex-init`
volume is shared, and should be initialized by the `security-bootstrapper`
container, which exposes its `/edgex-init` path into that volume. However,
as described in the github issue, Kubernetes does not treat volumes the
same way as docker-compose does, and the files inside the container are
not copied out to the shared volume on container startup. This results in
containers failing to start as their `command:` cannot be executed.

The workaround is implemented in `edgex/security-bootstrapper/Dockerfile`
(and `Dockerfile-arm`) by adding a line to `entrypoint.sh` to copy all the
files in `/edgex-init` to `/tmp/edgex-init` as shown below, and changing
the mount point of the volume for `security-bootstrapper` to
`/tmp/edgex-init`.

```
RUN sed -i -e '2 i cp -Rp /edgex-init/* /tmp/edgex-init' /entrypoint.sh
```

### `security-secretstore-setup` Reports "could not read master key shares file" Error (Issue 3852)

As described in the github issue, this problem arises from the `/vault/config`
being exposed as a volume, hiding the `/vault/config/assets` directory in the
container.

The workaround is implemented in `edgex/security-secretstore-setup`, using a
`Dockerfile` which adds commands to create the `assets` directory in
`/vault/config` when the container is started (in `entrypoint.sh`) as shown
below. This is sufficient since the `/vault/config` directory in the original
container only contains an empty `assets` directory and no other data.

```
RUN sed -i -e '2 i mkdir -p /vault/config/assets && chown -Rh 100:1000 /vault/' /usr/local/bin/entrypoint.sh
```

### Kong Crashes At Startup With Library Not Found Error

When the `kong` container starts the application itself at the end of the
`kong_wait_install.sh` script, the application fails with a missing library
error. This is caused by a similar problem to the two issues above. The
`/usr/local/kong` directory is exposed as a volume in order that the
`security-secretstore-setup` container can supply the `kong.yml` config
file for Kong's initialization. However, exposing the `/usr/local/kong` path
as a volume hides the `lib` directory underneath it, making it impossible
for the application to find and load its libraries.

The workaround is implemented in `edgex/security-bootstrapper/Dockerfile`
(and `Dockerfile-arm`) by patching the command in `kong_wait_install.sh` to
use `/tmp/kong/kong.yml` instead of `/usr/local/kong/kong.yml` as shown
below, and changing the deployment to mount the `kong` volume on `/tmp/kong`
in the `kong` container.

```
RUN sed -i -e 's|/usr/local/kong/kong.yml|/tmp/kong/kong.yml|g' ./kong_wait_install.sh
```

### Kong Admin API Returns Error

When the `security-proxy-setup` container attempts to create routes for the
service APIs using the Kong API, it receives an error. Examination of the
logs of the `kong` container revealed a failure occuring in the
`resolve_connect` function when attempting to resolve the hostname
"localhost". The container's `/etc/hosts` contains an appropriate entry
for "localhost" and the documentation of the function (`toip` from the
`resty.dns.client` package) that failed to find an entry indicates that both
`/etc/hosts` and a default "localhost" entry should be present.

The function `resolve_connect` is patched by Kong in the file
`/usr/local/share/lua/5.1/kong/globalpatches.lua`. We have found that calling
the `init` function from `resty.dns.client` before `toip` fixes the problem.
This is not an idea solution, and the problem requires further investigation.

The current workaround is to replace Kong's `globalpatches.lua` with a patched
version, calling `init` before `toip` in `edgex/kong/Dockerfile` (and
`Dockerfile-arm`):

```
COPY ./globalpatches.lua /usr/local/share/lua/5.1/kong/
```

### `security-proxy-setup` Container Restarts Constantly

The `security-proxy-setup` container runs to completion on success, and in
the EdgeX Foundry docker-compose file does not have the `restart: always`
notation applied to other containers. However, we have created a single
pod containing all the EdgeX Foundry containers and found that Kubernetes
does not support different restart policies for containers in the same
pod. As a result, the `security-proxy-setup` container appears to be
restarting constantly (and also causing unnecessary traffic on the Kong
API).

The workaround is implemented in the `edgex/security-bootstrapper/Dockerfile`
(and `Dockerfile-arm`), replacing the execution of
`security-proxy-setup --init=true` at the end of the
`proxy_setup_wait_install.sh` script to add an infinite loop of 15 second
sleep commands after the final command complete successfully.

```
RUN sed -i 's|exec /edgex/security-proxy-setup --init=true|/edgex/security-proxy-setup --init=true; until false; do sleep 15; done|g' ./proxy_setup_wait_install.sh
```
